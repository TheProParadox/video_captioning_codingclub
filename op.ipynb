{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/Shreyz-max/Video-Captioning\n",
    "### https://medium.com/analytics-vidhya/video-captioning-with-keras-511984a2cfff\n",
    "### https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import random\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import shutil\n",
    "import tqdm\n",
    "import cv2\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2b16a2efd0f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.85\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Loading json file for captions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Epil\\\\Desktop\\\\New folder\\\\MLDS_hw2_data\\\\training_label.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvocab_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "TRAIN_LABEL_PATH = 'C:\\\\Users\\\\Epil\\\\Desktop\\\\New folder\\\\MLDS_hw2_data\\\\training_label.json'\n",
    "# train test split\n",
    "split = 0.85\n",
    "# loading json file for captions\n",
    "with open(TRAIN_LABEL_PATH) as data_file:    \n",
    "    y_data = json.load(data_file)\n",
    "# train list contian all captions\n",
    "# vocab list contian all vocabulary for captions\n",
    "train_list = []\n",
    "vocab_list = []\n",
    "for y in y_data:\n",
    "    for caption in y['caption']:\n",
    "        caption = \"<bs> \" + caption + \" <es>\"\n",
    "        train_list.append([caption, y['id']])\n",
    "print(len(train_list))\n",
    "random.shuffle(train_list)\n",
    "training_list = train_list[:int(len(train_list)*split)]\n",
    "validation_list = train_list[int(len(train_list)*split):]\n",
    "for train in training_list:\n",
    "    vocab_list.append(train[0])\n",
    "# Tokenizing the words\n",
    "tokenizer = Tokenizer(num_words=1500)\n",
    "tokenizer.fit_on_texts(vocab_list)\n",
    "print(len(tokenizer.word_index))\n",
    "x_data = {}\n",
    "TRAIN_FEATURE_DIR = os.path.join('C:\\\\Users\\\\Epil\\\\Desktop\\\\New folder\\\\MLDS_hw2_data\\\\training_data', 'feat')\n",
    "# Loading all numpy arrays and saving them in a dictionary\n",
    "for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
    "    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n",
    "    x_data[filename[:-4]] = f\n",
    "print(len(training_list))\n",
    "print(len(validation_list))\n",
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom data generator\n",
    "def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=1500,training_list=train_list, batch_size=32, maxlen=10):\n",
    "    encoder_input_data = []\n",
    "    decoder_input_data = []\n",
    "    decoder_target_data = []\n",
    "    videoId = []\n",
    "    videoSeq = []\n",
    "    # separating the videoId and the captions\n",
    "    for idx, cap in enumerate(training_list):\n",
    "        caption = cap[0]\n",
    "        videoId.append(cap[1])\n",
    "        videoSeq.append(caption)\n",
    "    # converting the captions to tokens and padding them to equal sizes\n",
    "    train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
    "    train_sequences = np.array(train_sequences)\n",
    "    train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)\n",
    "    max_seq_length = train_sequences.shape[1]\n",
    "    filesize = len(train_sequences)\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    vCount = 0\n",
    "    n = 0\n",
    "    for i in range(epochs):\n",
    "        for idx in  range(0,filesize):\n",
    "            n += 1\n",
    "            encoder_input_data.append(x_data[videoId[idx]])\n",
    "            y = to_categorical(train_sequences[idx], num_decoder_tokens)\n",
    "            decoder_input_data.append(y[:-1])\n",
    "            decoder_target_data.append(y[1:])\n",
    "            if n == batch_size:\n",
    "                encoder_input = np.array(encoder_input_data)\n",
    "                decoder_input = np.array(decoder_input_data)\n",
    "                decoder_target = np.array(decoder_target_data)\n",
    "                encoder_input_data = []\n",
    "                decoder_input_data = []\n",
    "                decoder_target_data = []\n",
    "                n = 0\n",
    "                yield ([encoder_input, decoder_input], decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the train and validation generator\n",
    "train = load_datatest(train_path='training_data',batch_size=320, training_list=training_list, x_data=x_data, epochs=150)\n",
    "valid = load_datatest(train_path='training_data',batch_size=320, training_list=validation_list, x_data=x_data, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_encoder=80\n",
    "num_encoder_tokens=4096\n",
    "latent_dim=512\n",
    "time_steps_decoder=10\n",
    "num_decoder_tokens=1500\n",
    "batch_size=320\n",
    "epochs=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n",
    "encoder = LSTM(latent_dim, return_state=True,return_sequences=True, name='endcoder_lstm')\n",
    "_, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_relu')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 5, verbose=1, mode='min')\n",
    "# Run training\n",
    "opt = keras.optimizers.Adam(lr = 0.0003)\n",
    "x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n",
    "model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n",
    "try:\n",
    "    model.fit(train, validation_data=valid, validation_steps=(len(validation_list)//batch_size),\n",
    "        epochs=150, steps_per_epoch=(len(training_list)//batch_size),\n",
    "            callbacks=[x, earlystopping])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nW: interrupt received, stopping\")\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'model_final'\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "\n",
    "# Saving encoder as in training\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Saving decoder states and dense layer \n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "encoder_model.summary()\n",
    "decoder_model.summary()\n",
    "encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n",
    "decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n",
    "with open(os.path.join(save_model_path,'tokenizer'+ str(num_decoder_tokens) ),'wb') as file:\n",
    "    joblib.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to perform inference on all test files and save as test_output.txt\n",
    "class Video2Text(object):\n",
    "    ''' Initialize the parameters for the model '''\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 512\n",
    "        self.num_encoder_tokens = 4096\n",
    "        self.num_decoder_tokens = 1500\n",
    "        self.time_steps_encoder = 80\n",
    "        self.time_steps_decoder = None\n",
    "        self.preload = True\n",
    "        self.preload_data_path = 'preload_data'\n",
    "        self.max_probability = -1\n",
    "\n",
    "        # processed data\n",
    "        self.encoder_input_data = []\n",
    "        self.decoder_input_data = []\n",
    "        self.decoder_target_data = []\n",
    "        self.tokenizer = None\n",
    "\n",
    "        # models\n",
    "        self.encoder_model = None\n",
    "        self.decoder_model = None\n",
    "        self.inf_encoder_model = None\n",
    "        self.inf_decoder_model = None\n",
    "        self.save_model_path = 'model_final'\n",
    "        self.test_path = 'testing_data'\n",
    "    def load_inference_models(self):\n",
    "        # load tokenizer\n",
    "        \n",
    "        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n",
    "            self.tokenizer = joblib.load(file)\n",
    "\n",
    "        # inference encoder model\n",
    "        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, 'encoder_model.h5'))\n",
    "\n",
    "        # inference decoder model\n",
    "        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
    "        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.inf_decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n",
    "    \n",
    "    def decode_sequence2bs(self, input_seq):\n",
    "        states_value = self.inf_encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        self.beam_search(target_seq, states_value,[],[],0)\n",
    "        return decode_seq\n",
    "\n",
    "    def beam_search(self, target_seq, states_value, prob,  path, lens):\n",
    "        global decode_seq\n",
    "        node = 2\n",
    "        output_tokens, h, c = self.inf_decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        output_tokens = output_tokens.reshape((self.num_decoder_tokens))\n",
    "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
    "        states_value = [h, c]\n",
    "        for i in range(node):\n",
    "            if sampled_token_index[i] == 0:\n",
    "                sampled_char = ''\n",
    "            else:\n",
    "                sampled_char = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
    "            MAX_LEN = 10\n",
    "            if(sampled_char != 'eos' and lens <= MAX_LEN):\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                if(sampled_char == ''):\n",
    "                    p = 1\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                path_new = list(path)\n",
    "                path_new.append(sampled_char)\n",
    "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
    "                self.beam_search(target_seq, states_value, prob_new, path_new, lens+1)\n",
    "            else:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                p = functools.reduce(operator.mul, prob_new, 1)\n",
    "                if(p > self.max_probability):\n",
    "                    decode_seq = path\n",
    "                    self.max_probability = p\n",
    "\n",
    "    def decoded_sentence_tuning(self, decoded_sentence):\n",
    "        decode_str = []\n",
    "        filter_string = ['bos', 'eos']\n",
    "        unigram = {}\n",
    "        last_string = \"\"\n",
    "        for idx2, c in enumerate(decoded_sentence):\n",
    "            if c in unigram:\n",
    "                unigram[c] += 1\n",
    "            else:\n",
    "                unigram[c] = 1\n",
    "            if(last_string == c and idx2 > 0):\n",
    "                continue\n",
    "            if c in filter_string:\n",
    "                continue\n",
    "            if len(c) > 0:\n",
    "                decode_str.append(c)\n",
    "            if idx2 > 0:\n",
    "                last_string = c\n",
    "        return decode_str\n",
    "    \n",
    "    def get_test_data(self, path):\n",
    "        X_test = []\n",
    "        X_test_filename = []\n",
    "        with open (os.path.join('testing_id.txt')) as testing_file:         #path, \n",
    "            lines = testing_file.readlines()\n",
    "            for filename in lines:\n",
    "                filename = filename.strip()\n",
    "                f = np.load(os.path.join(path , 'feat', filename + '.npy'))\n",
    "                X_test.append(f)\n",
    "                X_test_filename.append(filename[:-4])\n",
    "            X_test = np.array(X_test)\n",
    "        return X_test, X_test_filename\n",
    "\n",
    "    def test(self):\n",
    "        X_test, X_test_filename = self.get_test_data(os.path.join(self.test_path))\n",
    "        print(len(X_test), len(X_test_filename))\n",
    "        # generate inference test outputs\n",
    "        with open(os.path.join(self.save_model_path, 'test_output.txt'), 'w') as file:\n",
    "            for idx, x in enumerate(X_test): \n",
    "                file.write(X_test_filename[idx]+',')\n",
    "                decoded_sentence = self.decode_sequence2bs(x.reshape(-1, 80, 4096))\n",
    "                decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
    "                for d in decode_str:\n",
    "                    file.write(d + ' ')\n",
    "                file.write('\\n')\n",
    "                # re-init max prob\n",
    "                self.max_probability = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Video2Text()\n",
    "c.load_inference_models()\n",
    "c.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_load():\n",
    "    model = VGG16(weights=\"imagenet\", include_top=True, input_shape=(224, 224, 3))\n",
    "    out = model.layers[-2].output\n",
    "    model_final = Model(inputs=model.input, outputs=out)\n",
    "    return model_final\n",
    "def extract_features(video, model):\n",
    "    \"\"\"\n",
    "    :param video: The video whose frames are to be extracted to convert into a numpy array\n",
    "    :param model: the pretrained vgg16 model\n",
    "    :return: numpy array of size 4096x80\n",
    "    \"\"\"\n",
    "    video_id = video.split(\".\")[0]\n",
    "    print(video_id)\n",
    "    print(f'Processing video {video}')\n",
    "\n",
    "    image_list = video_to_frames(video)\n",
    "    samples = np.round(np.linspace(\n",
    "        0, len(image_list) - 1, 80))\n",
    "    image_list = [image_list[int(sample)] for sample in samples]\n",
    "    images = np.zeros((len(image_list), 224, 224, 3))\n",
    "    for i in range(len(image_list)):\n",
    "        img = load_image(image_list[i])\n",
    "        images[i] = img\n",
    "    images = np.array(images)\n",
    "    fc_feats = model.predict(images, batch_size=128)\n",
    "    img_feats = np.array(fc_feats)\n",
    "    # cleanup\n",
    "    shutil.rmtree(os.path.join('/MLDS_hw2_data/testing_data', 'temporary_images'))\n",
    "    return img_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDescriptionRealTime(object):\n",
    "    \"\"\"\n",
    "        Initialize the parameters for the model\n",
    "        \"\"\"\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 512\n",
    "        self.num_encoder_tokens = 4096\n",
    "        self.num_decoder_tokens = 1500\n",
    "        self.time_steps_encoder = 80\n",
    "        self.max_probability = -1\n",
    "\n",
    "        # models\n",
    "        self.encoder_model = None\n",
    "        self.decoder_model = None\n",
    "        self.inf_encoder_model = None\n",
    "        self.inf_decoder_model = None\n",
    "        self.save_model_path = 'C:\\\\Users\\\\Epil\\\\Desktop\\\\New folder\\\\MLDS_hw2_data\\\\model_final'\n",
    "        self.test_path = 'C:\\\\Users\\\\Epil\\\\Desktop\\\\New folder\\\\MLDS_hw2_data\\\\testing_data'\n",
    "        self.search_type = 'greedy'\n",
    "        self.tokenizer = None\n",
    "        self.num = 0\n",
    "\n",
    "    def load_inference_models(self):\n",
    "        # load tokenizer\n",
    "\n",
    "        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n",
    "            self.tokenizer = joblib.load(file)\n",
    "\n",
    "        # inference encoder model\n",
    "        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, 'encoder_model.h5'))\n",
    "\n",
    "        # inference decoder model\n",
    "        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
    "        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.inf_decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n",
    "\n",
    "    def greedy_search(self, f):\n",
    "        \"\"\"\n",
    "        :param f: the loaded numpy array after creating videos to frames and extracting features\n",
    "        :return: the final sentence which has been predicted greedily\n",
    "        \"\"\"\n",
    "        inv_map = self.index_to_word()\n",
    "        states_value = self.inf_encoder_model.predict(f.reshape(-1, 80, 4096))\n",
    "        target_seq = np.zeros((1, 1, 1500))\n",
    "        final_sentence = ''\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        for i in range(15):\n",
    "            output_tokens, h, c = self.inf_decoder_model.predict([target_seq] + states_value)\n",
    "            states_value = [h, c]\n",
    "            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
    "            y_hat = np.argmax(output_tokens)\n",
    "            if y_hat == 0:\n",
    "                continue\n",
    "            if inv_map[y_hat] is None:\n",
    "                break\n",
    "            if inv_map[y_hat] == 'eos':\n",
    "                break\n",
    "            else:\n",
    "                final_sentence = final_sentence + inv_map[y_hat] + ' '\n",
    "                target_seq = np.zeros((1, 1, 1500))\n",
    "                target_seq[0, 0, y_hat] = 1\n",
    "        return final_sentence\n",
    "\n",
    "    def decode_sequence2bs(self, input_seq):\n",
    "        states_value = self.inf_encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        self.beam_search(target_seq, states_value, [], [], 0)\n",
    "        return decode_seq\n",
    "\n",
    "    def beam_search(self, target_seq, states_value, prob, path, lens):\n",
    "        \"\"\"\n",
    "        :param target_seq: the array that is fed into the model to predict the next word\n",
    "        :param states_value: previous state that is fed into the lstm cell\n",
    "        :param prob: probability of predicting a word\n",
    "        :param path: list of words from each sentence\n",
    "        :param lens: number of words\n",
    "        :return: final sentence\n",
    "        \"\"\"\n",
    "        global decode_seq\n",
    "        node = 2\n",
    "        output_tokens, h, c = self.inf_decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
    "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
    "        states_value = [h, c]\n",
    "        for i in range(node):\n",
    "            if sampled_token_index[i] == 0:\n",
    "                sampled_char = ''\n",
    "            else:\n",
    "                sampled_char = list(self.tokenizer.word_index.keys())[\n",
    "                    list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
    "            MAX_LEN = 12\n",
    "            if sampled_char != 'eos' and lens <= MAX_LEN:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                if sampled_char == '':\n",
    "                    p = 1\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                path_new = list(path)\n",
    "                path_new.append(sampled_char)\n",
    "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
    "                self.beam_search(target_seq, states_value, prob_new, path_new, lens + 1)\n",
    "            else:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                p = functools.reduce(operator.mul, prob_new, 1)\n",
    "                if p > self.max_probability:\n",
    "                    decode_seq = path\n",
    "                    self.max_probability = p\n",
    "\n",
    "    def decoded_sentence_tuning(self, decoded_sentence):\n",
    "        # tuning sentence\n",
    "        decode_str = []\n",
    "        filter_string = ['bos', 'eos']\n",
    "        uni_gram = {}\n",
    "        last_string = \"\"\n",
    "        for idx2, c in enumerate(decoded_sentence):\n",
    "            if c in uni_gram:\n",
    "                uni_gram[c] += 1\n",
    "            else:\n",
    "                uni_gram[c] = 1\n",
    "            if last_string == c and idx2 > 0:\n",
    "                continue\n",
    "            if c in filter_string:\n",
    "                continue\n",
    "            if len(c) > 0:\n",
    "                decode_str.append(c)\n",
    "            if idx2 > 0:\n",
    "                last_string = c\n",
    "        return decode_str\n",
    "\n",
    "    def index_to_word(self):\n",
    "        # inverts word tokenizer\n",
    "        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n",
    "        return index_to_word\n",
    "\n",
    "    def get_test_data(self):\n",
    "        # loads the features array\n",
    "        file_list = os.listdir(os.path.join(self.test_path, 'video'))\n",
    "        # with open(os.path.join(self.test_path, 'testing.txt')) as testing_file:\n",
    "            # lines = testing_file.readlines()\n",
    "        # file_name = lines[self.num].strip()\n",
    "        file_name = file_list[self.num]\n",
    "        path = os.path.join(self.test_path, 'feat', file_name + '.npy')\n",
    "        if os.path.exists(path):\n",
    "            f = np.load(path)\n",
    "        else:\n",
    "            model = model_cnn_load()\n",
    "            f = extract_features(file_name, model)\n",
    "        if self.num < len(file_list):\n",
    "            self.num += 1\n",
    "        else:\n",
    "            self.num = 0\n",
    "        return f, file_name\n",
    "\n",
    "    def test(self):\n",
    "        X_test, filename = self.get_test_data()\n",
    "        # generate inference test outputs\n",
    "        if self.search_type == 'greedy':\n",
    "            sentence_predicted = self.greedy_search(X_test.reshape((-1, 80, 4096)))\n",
    "        else:\n",
    "            sentence_predicted = ''\n",
    "            decoded_sentence = self.decode_sequence2bs(X_test.reshape((-1, 80, 4096)))\n",
    "            decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
    "            for d in decode_str:\n",
    "                sentence_predicted = sentence_predicted + d + ' '\n",
    "        # re-init max prob\n",
    "        self.max_probability = -1\n",
    "        return sentence_predicted, filename\n",
    "\n",
    "    def main(self, filename, caption):\n",
    "        \"\"\"\n",
    "        :param filename: the video to load\n",
    "        :param caption: final caption\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 1. Initialize reading video object\n",
    "        cap1 = cv2.VideoCapture(os.path.join(self.test_path, 'video', filename))\n",
    "        cap2 = cv2.VideoCapture(os.path.join(self.test_path, 'video', filename))\n",
    "        caption = '[' + ' '.join(caption.split()[1:]) + ']'\n",
    "        # 2. Cycle through pictures\n",
    "        while cap1.isOpened():\n",
    "            ret, frame = cap2.read()\n",
    "            ret2, frame2 = cap1.read()\n",
    "            if ret:\n",
    "                imS = cv2.resize(frame, (480, 300))\n",
    "                cv2.putText(imS, caption, (100, 270), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0),\n",
    "                            2, cv2.LINE_4)\n",
    "                cv2.imshow(\"VIDEO CAPTIONING\",imS)  \n",
    "            if ret2:\n",
    "                imS = cv2.resize(frame, (480, 300))\n",
    "                cv2.imshow(\"ORIGINAL\",imS)    \n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # Quit playing\n",
    "            key = cv2.waitKey(25)\n",
    "            if key == 27:  # Button esc\n",
    "                break\n",
    "\n",
    "        # 3. Free resources\n",
    "        cap1.release()\n",
    "        cap2.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_to_text = VideoDescriptionRealTime()\n",
    "    video_to_text.load_inference_models()\n",
    "    while True:\n",
    "        print('.........................\\nGenerating Caption:\\n')\n",
    "        start = time.time()\n",
    "        video_caption, file = video_to_text.test()\n",
    "        end = time.time()\n",
    "        sentence = ''\n",
    "        print(sentence)\n",
    "        for text in video_caption.split():\n",
    "            sentence = sentence + ' ' + text\n",
    "            print('\\n.........................\\n')\n",
    "            print(sentence)\n",
    "        print('\\n.........................\\n')\n",
    "        print('It took {:.2f} seconds to generate caption'.format(end-start))\n",
    "        video_to_text.main(file, sentence)\n",
    "        play_video = input('Should I play the video? ')\n",
    "        if play_video.lower() == 'y':\n",
    "            continue\n",
    "        elif play_video.lower() == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print('Could not understand type (y) for yes and (n) for no')\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
